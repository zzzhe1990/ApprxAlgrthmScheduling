****************************************
Nov 27 2016
****************************************
Complete:
1. Initialize the device's array size of NSsubsets and optVector.
2. Initialize some device variables: dev_AllTableElemets_size, powK, 
3. Fix the problem of using incorrect sizes. In gpu_generate2(), the second parameter should be pow(k,2) instead of dev_ATE_elm_size.
4. Fix the problem of memory copy to device. Move memcpy() out of while() loop, which is in DPFunction2().
5. Add memcpy() from device to host for arrays: dev_counterVec, dev_ATE_NSsubsets, dev_ATE_optVector, dev_ATE_Csubsets. Copy these device arrays into "AllTableElemets" object.

Problem:
1. What is the real size of Csubsets()? Have to check the iteration of generate2() and CWhole.size() to see if the data size in 
	cudaMemcpy(AllTableElemets[i].Csubsets, &dev_ATE_Csubsets, DeviceToHost) is correct.



****************************************
Nov 28 2016
****************************************
Fixed Problem:
1. Nov 27 2016, Problem 1: Csubsets has a size of [Cwhole.size()][pow(k,2)]

Complete:
1. declaration of GPU functions.
2. first draft of initialization for all GPU functions.
3. update comments to make array sizes more explicit.

To do:
1. Compile code and debug.



****************************************
Dec 1 2016
****************************************
Complete:
1. Create header files for .cpp, so that .cu can include the header file to identify vector.
2. Move all cudaMalloc and cudaMemcpy to .cu file.
3. Move the While loop from DPFunction2 (in .cpp originally) to .cu.
4. Fix bugs on parameters.

Add:
1. makefile

Problem:
1. bugs about ifstream.
2. makefile problem.


****************************************
Dec 5 2016
****************************************
Complete:
1. solve the error of multiple definition
2. add memory copy from host to device for "counterVec"
3. correct the calculation on block size "bSize"
4. Make the code compiled correctly


****************************************
Dec 7 2016
****************************************
Complete:
1. Change the size of "optVec" and "NSsubsets" from fix value to dynamic. Size for mallocation is still fixed, but an array of size for different dimensionns are also recorded.
2. Add cudafree() for optVec_size and NSsubsets_size.
3. Change the size of cudaMemcpy for dev_counterVec from device to host. 


****************************************
Dec 12 2016
****************************************
Complete:
1. Address the problem of incorrect cudaMemcpy, caused by multi-dimensional vector. Copy from CUDA container to STL container directly may cause problems especially when 
   STL container is in multi-dimensions and not allocated.
2. Add cudaMemcpy Device to Host for myOPT and myOptimalindex. 
3. Change the way of copying between host and device; however, there is still heavy overhead on moving data from device (CUDA array) to host (STL vectors).

To do:
1. Run the original code on GRID to collect data and compare to GPU results.
2. Try to put all DPFunction2 on GPU to minimize the cost of communication.

Errors:
1. It shows that AllTableElemets[i].myOPT is always 1, which is not correct.

for(int i=0; i<NSTableElements.size();i++)			//NSTableElements is N - S. For example. (2,3), si = (0,1), then NS[i] = (2,2)
	{
		for(int j=0;j<AllTableElemets.size();j++)
		{	
			if(NSTableElements[i].elm==AllTableElemets[j].elm)
			{
				NSTableElements[i]=AllTableElemets[j];
				break;
			}
		}

	}


	for(int i=0; i<NSTableElements.size();i++)
	{
		tempOptVector.push_back(NSTableElements[i].myOPT);
	}

	int dpoptimal;
	int minN=100000;
    cout << "NSTableElements size: " << NSTableElements.size() << endl;  
	for(int mindex=0; mindex<NSTableElements.size();mindex++)
	{
		cout << "index: " << mindex << ", NSTableElements.myOPT: " << NSTableElements[mindex].myOPT << ", minN: " << minN << endl;
		if(NSTableElements[mindex].myOPT<minN)
			minN=NSTableElements[mindex].myOPT;
	}

	dpoptimal=minN +1;

*********************************************
Dec 17 2016
*********************************************
Problem solved:
1. Have to copy AllTableElemets[i].myOPT to GPU because it is not initialized to 0 but the number of total jobs.

Problem detected and still remain:
1. The update of optVector always hits the condition of NSsubsets == zeroVec. Problem might be caused by the updates on NSsubsets that some reserved elements (initial to 0) 
	are not updated (should not be used) but they are somehow used or compared to in GPU code. In cpp code, since it is a vector, there is no redudent elements, but in GPU code,
	size of arrays are reserved to MAX.
	
			if(gpu_sameVectors(&dev_ATE_NSsubsets[(j * Cwhole_size + h) * powK], dev_zeroVec, powK))
                {
                    //AllTableElemets[j].optVector.push_back(0);
                    dev_ATE_optVector[j * powK + optVecIndex] = 0;
                    optVecIndex++;
                    dev_ATE_optVector_size[j] = optVecIndex;
                    hit1++;
                    break;
                }
                
                
*********************************************
Dec 20
*********************************************
New found:
1. AllTableElemets[j].NSsubsets.size() is not constant, it is not the same to  AllTableElemets.size(). Find AllTableElemets[max].elm, for example: it is 0 0 5 2 3 0 6 0 0. Thus max NSsubsets.size()
   is 6 + 6*3 + 6*3*2 + 6*3*2*5. This value can be calculated between generate() and generate2().
2. AllTableElemets[j].NSsubsets[xxx].size() is constant to pow(k,2).
3. AllTableElemets[j].elm.size() is constant to pow(k,2).
